---
title: "Bikeshare Forecasting"
author: "Ashic Mahtab"
date: "Wednesday, August 20, 2014"
output: html_document
---
```{r echo=FALSE}
library(caret)
library(gbm)
library(Cubist)
```

# Summary
This is a report of applying machine learning to a Bike Sharing data set, and making predictions. Given a few attributes, and a moderate amount of data, with the response being the number of bike hires in particular hours, we are to make predictions for dates for which we only have feature data. 

The training data set has 10886 entries with small amount of data in each entry. As such, the entire data set can comfortably fit in ram. For this reason, I have chosen to use R, and not some distributed processing engine. 

I started off with a simple model working on a cleaned up version of the input data. I used simple linear regression modeling for this. From there, I added secondary features (gained from explorartory analysis), and applied boosting with stochastic gradient boosting (gbm) and cubist. I split the training set into a training and cross validation set. I saw that each model progressively resuced the squared errors on the validation set. I assumed the models would progressively improve results.

With the models at hand, I trained them with the complete training set, before making the predictions. I submitted the predictions to Kaggle, and the results were very similar to expected result gained from the model selection phase. While the second linear regression model increased the error slightly (likely due to overfitting), the boosted models fared significantly better. The cubist model (identified as the best performing) got the best result, and took me to position 208 on the leaderboard. Additional methods like random forests, and ensembles of models may improve the prediction accuracy further, and are the next logical steps to try.

# Prerequisites
The code assume the train.csv and test.csv files are in the working directory. It uses the caret, gbm and Cubist packages. 

# Cleaning Data
As a firt step, we load the input data:
``` {r cache=TRUE}
input <- read.csv('train.csv', colClasses = c('character', rep('integer', 4), rep('numeric', 4), rep('integer', 3)))
str(input)
```

Since we have features for season, and whether the day is a holiday, working day, etc., we can assume we do not need to consider the particular date of a record - however we are interested in the hour. 
``` {r cache=TRUE}
input$hour <- as.integer(substr(input$datetime, 12, 13))
input <- input[, -1]
input <- input[, c(12, 1:11)]
```

# Exploratory Analysis
We start off with a pairs plot of the input data.

```{r cache=TRUE}
pairs(input, panel=panel.smooth)
```

From the plots, we can see that atemp and temp provide almost exact responses with atemp being affected by other factors in a slightly stronger manner. There appears to be a quadratic relationship between count, and atemp and hour. The responses for casual and registered seem very similar as well. As such, we can consider just counts. Our reduced dataset is given by:

``` {r cache=TRUE}
set.seed(1143)
input <- input[, -c(6, 10, 11)]
inTrain <- createDataPartition(y=input$count, p=0.7, list=F)
training <- input[inTrain, ]
validation <- input[-inTrain, ]
```

# Model Selection
We start off with a maximal model:

```{r cache=TRUE}
set.seed(1143)
mdl<- train(count ~ ., data=training, method='lm')
p <- predict(mdl, validation)
p[p<0] <- 0
s <- sqrt(sum((p-validation$count)^2))
s
```

We can leverage the quadratic relationship of count with atemp and hour to improve the model:

```{r cache=TRUE}
set.seed(1143)
mdl2 <- train(count ~ . + I(atemp^2) + I(hour^2), data=training, method='lm')
p2 <- predict(mdl2, validation)
p2[p2<0] <- 0
s2 <- sqrt(sum((p2-validation$count)^2))
s2
```

We can apply stochastic gradient boosting:

```{r cache=TRUE}
set.seed(1143)
mdl3 <- train(count ~ . + I(atemp^2) + I(hour^2), data=training, method='gbm', verbose=F)
p3 <- predict(mdl3, validation)
p3[p3<0] <- 0
s3 <- sqrt(sum((p3-validation$count)^2))
s3
```

The boosting has reduced the error on the validation set. We can try out another boosting algorithm, Cubist.
```{r cache=TRUE}
set.seed(1143)
mdl4 <- train(count ~ . + I(atemp^2) + I(hour^2), data=training, method='cubist')
p4 <- predict(mdl4, validation)
p4[p4<0] <- 0
s4 <- sqrt(sum((p4-validation$count)^2))
s4
```

This has given the least error on the validation set.

# Applying the Model
We have our models, of which mdl3 and mdl4 perform the best. We can now improve our models a bit by training them with the entire training data set (not just the training set we chose from the provided training set). The more data we have to train them with, the better!

```{r cache=TRUE}
mdlLm <- train(count ~ ., data=input, method='lm')
mdlLm2 <- train(count ~ . + I(atemp^2) + I(hour^2), data=input, method='lm')
mdlGbm <- train(count ~ . + I(atemp^2) + I(hour^2), data=input, method='gbm', verbose=F)
mdlCubist <- train(count ~ . + I(atemp^2) + I(hour^2), data=input, method='cubist')
```

With the models ready and trained, we load in the test data, and transform it to suit the form our models expect.

```{r cache=TRUE}
testSet <- read.csv('test.csv', colClasses = c('character', rep('integer', 4), rep('numeric', 4)))
testSet$hour <- as.integer(substr(testSet$datetime, 12, 13))
```

The following function makes predictions from a model:

```{r cache=TRUE}
runPrediction <- function (mdl) {
    p <- predict(mdl, testSet)
    p <- round(p)
    p[p<0] <- 0
    df <- data.frame(testSet$datetime, c(count=p))
    names(df) <- c('datetime', 'count')
    df
}

```

We then use the function to make the predictions:

```{r cache=TRUE}
predLm <- runPrediction(mdlLm)
predLm2 <- runPrediction(mdlLm2)
predGbm <- runPrediction(mdlGbm)
predCubist <- runPrediction(mdlCubist)
```

Finally, we can write out the results:

```{r cache=TRUE}
write.csv(predLm, 'predLm.csv', quote=F, row.names=F)
write.csv(predLm2, 'predLm2.csv', quote=F, row.names=F)
write.csv(predGbm, 'predGbm.csv', quote=F, row.names=F)
write.csv(predCubist, 'predCubist.csv', quote=F, row.names=F)
```

# Results
On submitting the results, Kaggle gave us the following results:

LM: 1.26560
LM2: 1.32560
GBM: 0.80568
Cubist: 0.50568

(Lower is better)

It appears that on the test set, LM performed marginally better than LM2. This suggests a bit of overfitting in our model may be present. Applying k fold cross validation, boosting, etc. may help. The boosted models performed significantly better, reducing the RMSLE to just over 0.5 in the cubist model.

Additional methods like random forests, and ensembles of different models may improve the score even more. Those will be the next steps.
